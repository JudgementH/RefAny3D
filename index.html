<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MVTokenFlow for high-quality 4D content creation from monocular videos.">
  <meta name="keywords" content="Diffusion, Gaussians, 4D content creation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</h1>
            <h4 style="color:#5a6268;">ICLR 2026</h4>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/JudgementH">Hanzhuo Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.utoronto.ca/~qingyangbao/">Qingyang Bao</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Y8AU3RkAAAAJ&hl=en">Zekai Gu</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="">Zhongshuo Du</a><sup>5</sup>,
              </span>
              <span class="author-block">
                <a href="https://clinplayer.github.io/">Cheng Lin</a><sup>6</sup>,
              </span>
              <span class="author-block">
                <a href="https://liuyuan-pal.github.io/">Yuan Liu</a><sup>4&dagger;</sup>,
              </span>
              <span class="author-block">
                <a href="https://sibeiyang.github.io/">Sibei Yang</a><sup>2&dagger;</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ShanghaiTech,</span>
              <span class="author-block"><sup>2</sup>SYSU,</span>
              <span class="author-block"><sup>3</sup>U of T,</span>
              <span class="author-block"><sup>4</sup>HKUST,</span>
              <span class="author-block"><sup>5</sup>SynWorld,</span>
              <span class="author-block"><sup>6</sup>MUST</span>
            </div>

            <sup>&dagger;</sup>Corresponding author.

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- arXiv Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="hero">
    <div class="container is-max-desktop is-centered">
      <img src="./static/images/teaser.png" alt="teaser image.">
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.
            </p>
          </div>
        </div>
      </div>

    </div>
  </section>

  <!-- Method -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content">
          <h2 class="title is-3">Method</h2>
          <img src="./static/images/method.png" alt="method." />
          <div class="content has-text-justified">
          <p>
            <b>Overview of RefAny3D.</b> Given a 3D asset, we render multi-view inputs as conditioning signals for the diffusion model and simultaneously generate the point map of the target RGB image.  To ensure pixel-level consistency across different viewpoints, we adopt a shared positional encoding strategy. Moreover, to disentangle the RGB domain from the point map domain, we incorporate Domain-specific LoRA and Text-agnostic Attention. Benefiting from this 3D-aware disentangle ment design, our method is able to generate high-quality images that maintain strong consistency with the underlying 3D assets.
          </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Data Pipeline -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content">
          <h2 class="title is-3">Data Construction Pipeline</h2>
          <img src="./static/images/data_pipeline.png" alt="data pipeline." />
          <div class="content has-text-justified">
          <p>
            <b>(a)</b> Data construction pipeline. We first use GroundingDINO to extract the objects of interest, then convert the images into 3D models using Hunyuan3D, and finally apply FoundationPose to estimate the poses of the 3D models in the images. <b>(b)</b> Examples from the dataset.
          </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content">
          <h2 class="title is-3">Qualitative Results</h2>
          <img src="./static/images/qualitative.png" alt="qualitative results." />
          <div class="content has-text-justified">
          <p>
            Qualitative comparison with other methods.
          </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content">
          <h2 class="title is-3">RGB and Point Map</h2>
          <img src="./static/images/pointmap.png" alt="point map." />
          <div class="content has-text-justified">
          <p>
            Qualitative results with different 3D assets as references. Our method takes a given 3D mesh as input and generates both RGB images and point maps in a unified manner. By enforcing pixel-level spatial alignment between the point maps and RGB outputs, the framework ensures consistent geometry–texture correspondence across views. Moreover, the incorporation of point maps enhances the model’s 3D structural awareness, thereby improving the fidelity and consistency of image generation with respect to the reference 3D assets.
          </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content">
          <h2 class="title is-3">Ablation Study</h2>
          <img src="./static/images/ablation.png" alt="ablation study." />
          <div class="content has-text-justified">
          <p>
            Ablation studies on different components of our method: (a) full model; (b) without Shared Positional Embedding for Cross-Domain; (c) without Text-agnostic Attention; (d) without Domain-specific LoRA.
          </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content">
          <img src="./static/images/ablation.png" alt="ablation study." />
          <div class="content has-text-justified">
          <p>
            Comparisons of ablation studies and the editing-based baseline.
          </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>

      </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              We thank the <a herf="https://github.com/nerfies/nerfies.github.io">Nerfies</a> team for providing this
              amazing website template.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>